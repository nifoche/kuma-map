#!/usr/bin/env python3
"""
ç†Šå‡ºæ²¡ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

1. Google News RSSã‹ã‚‰ã€Œç†Š å‡ºæ²¡ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—
2. Claude APIã§ä½ç½®æƒ…å ±ã‚’æŠ½å‡º
3. å›½åœŸåœ°ç†é™¢APIã§åº§æ¨™ã«å¤‰æ›
4. Supabaseã«ä¿å­˜
5. æ–°è¦ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°Slacké€šçŸ¥
"""

import json
import os
import re
import hashlib
from datetime import datetime
from supabase import create_client, Client

import requests
from anthropic import Anthropic

# APIè¨­å®š
ANTHROPIC_API_KEY = os.environ.get("ANTHROPIC_API_KEY")
SLACK_WEBHOOK_URL = os.environ.get("SLACK_WEBHOOK_URL_TANALABO")

# Supabaseè¨­å®š
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")

# Google News RSS URL
NEWS_RSS_URL = "https://news.google.com/rss/search?q=ç†Š+å‡ºæ²¡&hl=ja&gl=JP&ceid=JP:ja"


def get_supabase_client() -> Client | None:
    """Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å–å¾—"""
    if not SUPABASE_URL or not SUPABASE_KEY:
        print("Supabaseè¨­å®šãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None
    return create_client(SUPABASE_URL, SUPABASE_KEY)


def fetch_news() -> list[dict]:
    """Google News RSSã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"""
    import xml.etree.ElementTree as ET

    try:
        response = requests.get(NEWS_RSS_URL, timeout=30)
        response.raise_for_status()

        root = ET.fromstring(response.content)
        items = []

        for item in root.findall(".//item")[:10]:  # æœ€æ–°10ä»¶
            title = item.find("title").text if item.find("title") is not None else ""
            link = item.find("link").text if item.find("link") is not None else ""
            pub_date = item.find("pubDate").text if item.find("pubDate") is not None else ""

            # ç†Šé–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            if "ç†Š" in title or "ã‚¯ãƒ" in title:
                items.append({
                    "title": title,
                    "link": link,
                    "pub_date": pub_date
                })

        return items
    except Exception as e:
        print(f"ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return []


def extract_location_with_claude(news_items: list[dict]) -> list[dict]:
    """Claude APIã§ä½ç½®æƒ…å ±ã‚’æŠ½å‡º"""
    if not ANTHROPIC_API_KEY:
        print("ANTHROPIC_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return []

    client = Anthropic(api_key=ANTHROPIC_API_KEY)
    results = []

    for item in news_items:
        try:
            prompt = f"""ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰ç†Šã®å‡ºæ²¡æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚

ã‚¿ã‚¤ãƒˆãƒ«: {item['title']}

ä»¥ä¸‹ã®JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆæ—¥æœ¬èªã§ï¼‰ï¼š
{{
  "prefecture": "éƒ½é“åºœçœŒåï¼ˆä¾‹ï¼šç§‹ç”°çœŒï¼‰",
  "city": "å¸‚åŒºç”ºæ‘åï¼ˆä¾‹ï¼šç§‹ç”°å¸‚ï¼‰",
  "location": "è©³ç´°ãªåœ°åï¼ˆä¾‹ï¼šé›„å’Œåœ°åŒºï¼‰ã€‚ä¸æ˜ãªå ´åˆã¯ç©ºæ–‡å­—",
  "summary": "å‡ºæ²¡æƒ…å ±ã®è¦ç´„ï¼ˆ50æ–‡å­—ä»¥å†…ï¼‰",
  "is_bear_sighting": true ã¾ãŸã¯ falseï¼ˆç†Šå‡ºæ²¡ã«é–¢ã™ã‚‹æƒ…å ±ã‹ã©ã†ã‹ï¼‰
}}

ç†Šã®å‡ºæ²¡æƒ…å ±ã§ãªã„å ´åˆã¯ is_bear_sighting ã‚’ false ã«ã—ã¦ãã ã•ã„ã€‚
JSONã®ã¿ã‚’å‡ºåŠ›ã—ã€ä»–ã®æ–‡ç« ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚"""

            response = client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=300,
                messages=[{"role": "user", "content": prompt}]
            )

            # JSONæŠ½å‡º
            content = response.content[0].text.strip()
            # JSONãƒ–ãƒ­ãƒƒã‚¯ã‚’æŠ½å‡º
            json_match = re.search(r'\{[\s\S]*\}', content)
            if json_match:
                data = json.loads(json_match.group())
                if data.get("is_bear_sighting", False) and data.get("prefecture"):
                    data["source"] = item["link"]
                    data["pub_date"] = item["pub_date"]
                    results.append(data)
                    print(f"æŠ½å‡ºæˆåŠŸ: {data['prefecture']} {data['city']}")
        except Exception as e:
            print(f"Claude API ã‚¨ãƒ©ãƒ¼: {e}")
            continue

    return results


def geocode_location(prefecture: str, city: str, location: str = "") -> tuple[float, float] | None:
    """å›½åœŸåœ°ç†é™¢APIã§ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°"""
    query = f"{prefecture}{city}{location}"

    try:
        url = "https://msearch.gsi.go.jp/address-search/AddressSearch"
        params = {"q": query}
        response = requests.get(url, params=params, timeout=10)
        response.raise_for_status()

        results = response.json()
        if results and len(results) > 0:
            # æœ€åˆã®çµæœã‚’ä½¿ç”¨
            coords = results[0]["geometry"]["coordinates"]
            return (coords[1], coords[0])  # lat, lng
    except Exception as e:
        print(f"ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ ({query}): {e}")

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å¸‚åŒºç”ºæ‘ã®ã¿ã§å†è©¦è¡Œ
    if location:
        return geocode_location(prefecture, city, "")

    return None


def get_existing_ids(supabase: Client) -> set[str]:
    """æ—¢å­˜ã®IDã‚’Supabaseã‹ã‚‰å–å¾—"""
    try:
        response = supabase.table("bear_sightings").select("id").execute()
        return {row["id"] for row in response.data}
    except Exception as e:
        print(f"æ—¢å­˜IDå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        return set()


def save_to_supabase(supabase: Client, sightings: list[dict]) -> int:
    """Supabaseã«ä¿å­˜"""
    if not sightings:
        return 0

    # ã‚«ãƒ©ãƒ åã‚’ã‚¹ãƒãƒ¼ã‚¯ã‚±ãƒ¼ã‚¹ã«å¤‰æ›
    rows = []
    for s in sightings:
        rows.append({
            "id": s["id"],
            "date": s["date"],
            "prefecture": s["prefecture"],
            "city": s["city"],
            "location": s.get("location", ""),
            "lat": s["lat"],
            "lng": s["lng"],
            "source": s.get("source", ""),
            "summary": s.get("summary", ""),
        })

    try:
        response = supabase.table("bear_sightings").insert(rows).execute()
        return len(response.data)
    except Exception as e:
        print(f"Supabaseä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        return 0


def generate_id(prefecture: str, city: str, date: str) -> str:
    """é‡è¤‡ãƒã‚§ãƒƒã‚¯ç”¨ã®IDç”Ÿæˆ"""
    content = f"{prefecture}{city}{date}"
    return hashlib.md5(content.encode()).hexdigest()[:12]


def send_slack_notification(new_sightings: list[dict]):
    """Slacké€šçŸ¥ã‚’é€ä¿¡"""
    if not SLACK_WEBHOOK_URL or not new_sightings:
        return

    count = len(new_sightings)
    locations = "\n".join([
        f"â€¢ {s['prefecture']} {s['city']}: {s['summary']}"
        for s in new_sightings[:5]  # æœ€å¤§5ä»¶
    ])

    message = {
        "text": f"ğŸ» ç†Šå‡ºæ²¡æƒ…å ±: {count}ä»¶ã®æ–°ã—ã„æƒ…å ±ã‚’è¿½åŠ ã—ã¾ã—ãŸ",
        "attachments": [{
            "color": "#FF6B6B",
            "fields": [
                {
                    "title": "æ–°è¦å‡ºæ²¡æƒ…å ±",
                    "value": locations,
                    "short": False
                },
                {
                    "title": "ç¢ºèª",
                    "value": "<https://kuma-map.netlify.app|ç†Šå‡ºæ²¡ãƒãƒƒãƒ—ã§ç¢ºèª>",
                    "short": False
                }
            ]
        }]
    }

    try:
        response = requests.post(SLACK_WEBHOOK_URL, json=message, timeout=10)
        response.raise_for_status()
        print(f"Slacké€šçŸ¥é€ä¿¡å®Œäº†: {count}ä»¶")
    except Exception as e:
        print(f"Slacké€šçŸ¥ã‚¨ãƒ©ãƒ¼: {e}")


def main():
    print("=== ç†Šå‡ºæ²¡ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†é–‹å§‹ ===")
    print(f"å®Ÿè¡Œæ—¥æ™‚: {datetime.now().isoformat()}")

    # Supabaseã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    supabase = get_supabase_client()
    if not supabase:
        print("Supabaseæ¥ç¶šã«å¤±æ•—ã—ã¾ã—ãŸ")
        return

    # 1. ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—
    print("\n1. ãƒ‹ãƒ¥ãƒ¼ã‚¹å–å¾—ä¸­...")
    news_items = fetch_news()
    print(f"   å–å¾—ä»¶æ•°: {len(news_items)}")

    if not news_items:
        print("ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return

    # 2. ä½ç½®æƒ…å ±æŠ½å‡º
    print("\n2. Claude APIã§ä½ç½®æƒ…å ±æŠ½å‡ºä¸­...")
    extracted = extract_location_with_claude(news_items)
    print(f"   æŠ½å‡ºä»¶æ•°: {len(extracted)}")

    if not extracted:
        print("æŠ½å‡ºå¯èƒ½ãªæƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        return

    # 3. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿å–å¾—
    print("\n3. æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ç¢ºèªä¸­...")
    existing_ids = get_existing_ids(supabase)
    print(f"   æ—¢å­˜ä»¶æ•°: {len(existing_ids)}")

    # 4. ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼†æ–°è¦ãƒ‡ãƒ¼ã‚¿è¿½åŠ 
    print("\n4. ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼†ãƒ‡ãƒ¼ã‚¿è¿½åŠ ä¸­...")
    new_sightings = []
    today = datetime.now().strftime("%Y-%m-%d")

    for item in extracted:
        # IDç”Ÿæˆ
        sighting_id = generate_id(item["prefecture"], item["city"], today)

        if sighting_id in existing_ids:
            print(f"   ã‚¹ã‚­ãƒƒãƒ—ï¼ˆé‡è¤‡ï¼‰: {item['prefecture']} {item['city']}")
            continue

        # ã‚¸ã‚ªã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°
        coords = geocode_location(item["prefecture"], item["city"], item.get("location", ""))
        if not coords:
            print(f"   ã‚¹ã‚­ãƒƒãƒ—ï¼ˆåº§æ¨™å–å¾—å¤±æ•—ï¼‰: {item['prefecture']} {item['city']}")
            continue

        # æ–°è¦ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        new_sighting = {
            "id": sighting_id,
            "date": today,
            "prefecture": item["prefecture"],
            "city": item["city"],
            "location": item.get("location", ""),
            "lat": coords[0],
            "lng": coords[1],
            "source": item.get("source", ""),
            "summary": item.get("summary", ""),
        }

        new_sightings.append(new_sighting)
        existing_ids.add(sighting_id)
        print(f"   è¿½åŠ : {item['prefecture']} {item['city']}")

    # 5. Supabaseã«ä¿å­˜
    if new_sightings:
        print(f"\n5. Supabaseã«ä¿å­˜ä¸­... ({len(new_sightings)}ä»¶)")
        saved = save_to_supabase(supabase, new_sightings)
        print(f"   ä¿å­˜å®Œäº†: {saved}ä»¶")

        # 6. Slacké€šçŸ¥
        print("\n6. Slacké€šçŸ¥é€ä¿¡ä¸­...")
        send_slack_notification(new_sightings)
    else:
        print("\næ–°è¦ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

    print("\n=== å®Œäº† ===")
    print(f"ç·ä»¶æ•°: {len(existing_ids)}")


if __name__ == "__main__":
    main()
